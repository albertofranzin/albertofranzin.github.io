<!doctype html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8">
    <title>ARIAC WP1, 29 March, online</title>
    <link rel="stylesheet" type="text/css" href="../css/normalize.css">
    <link rel="stylesheet" type="text/css" href="../css/style.css">
    <link rel="icon" href="../img/favicon.ico" type="image/gif"> 
    <style>
    h1,h2 { margin-left: 0px; }
    </style>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <base target="_blank">
  </head>
  <body>
    <div id="upperempty"> </div>
    <div class="wrapper" id="centerdiv">

        <div id="titlename">
           <h1>Second 2022/23 ARIAC WP1 meeting, 29/03/2023, online</h1>
        </div>

<p>
The second ARIAC WP1 meeting of the 2022/23 academic year will take place on Wednesday 29 March 2023 morning. The focus of this meeting is the task 1.5 on Explainable AI. We will have both invited presentations and presentations from TRAIL/ARIAC researchers about their work.
</p>
<p>For the recording of the meeting, please contact alberto.franzin at ulb dot be.
</p>

<h3>Invited speaker: prof. <a href="https://www.uantwerpen.be/en/staff/david-martens/">David Martens</a> (UAntwerpen)</h3>
<h3>The Counterfactual Explanation: Yet More Algorithms as a Solution to Explain Complex Models?</h3>
<p>
Abstract:
The inability of many "black box" prediction models to explain the decisions made, have been widely acknowledged. Interestingly, the solution turns out to be the introduction of yet more AI algorithms that explain the decisions made by complex AI models. Explaining the predictions of such models has become an important ethical component and has gained increased attention of the AI research community and even legislator, resulting in a new field termed "Explainable AI" (XAI). Counterfactual (CF) explanations has emerged as an important paradigm in this field, and provides evidence on how a (1) prediction model came to a (2) decision for a (3) specific data instance. In this talk, I'll first provide an introduction to the counterfactual explanation and compare it to other popular XAI approaches. Next, some counterfactual generating techniques are discussed for tabular, textual, behavioral and image data, accompanied with some example applications, demonstrating the value in a range of areas.
</p>
<p>
Bio:
David Martens is Professor of Data Science at the University of Antwerp, Belgium. He teaches data mining and data science and ethics to postgraduate students studying business economics and business engineering. David's work with Foster Provost on instance-based explanations is regarded as one of the first to have introduced counterfactual explanations to the AI domain. His research has been published in high-impact journals and has received several awards. David is also the author of the book "Data Science Ethics: Concepts, Techniques and Cautionary Tales", published by Oxford University Press.
</p>

<h2>Meeting program</h2>
<ul>
<li>9:00 - 9:05 Alberto Franzin: introduction</li>
<li>9:05 - 9:25 Lucile Dierckx: RL-Net: Interpretable Rule Learning with Neural Networks</li>
<li>9:25 - 9:45 Julien Albert: Bridging the Gap between Human-Computer Interaction and Machine-Learning on Explainable AI: Initial Observations and Lessons Learned (<a href="slides/20230329_presentationWP1XAI.pdf">slides</a>, <a href="https://projects.info.unamur.be/ihm-xai/">IHM'22 workshop</a>)</li>
<li>9:45 - 10:05 J&eacute;r&eacute;mie Bogaert: TIPECS : A corpus cleaning method using machine learning and qualitative analysis</li>
<li>10:05 - 10:15 break</li>
<li>10:15 - 11:15 David Martens: The Counterfactual Explanation: Yet More Algorithms as a Solution to Explain Complex Models? (invited talk)</li>
</ul>

<h2>Contact</h2>
<p>
For any question, etc, please contact alberto.franzin at ulb dot be.
</p>
<p>The <a href="index.html" target="_self">list</a> of all the WP1 meetings.</p>

<div id="lastupdate">Last update: 2023.03.29</div>
</div> 

</body>
</html>
